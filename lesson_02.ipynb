{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Importing Data Into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn to pull data into our Python environment and run some of the first diagnostics we'll need to understand our data.\n",
    "\n",
    "Why don't you kick us off by importing our two most important data analytic tools into our Jupyter notebook: Pandas and Numpy?\n",
    "\n",
    "## Exercise 1\n",
    "Import Pandas and NumPy into your Jupyter Notebook and assign the standard aliases to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go straight into importing data into our Python environment! Check out DC's Open Data Portal at opendata.dc.gov.\n",
    "\n",
    "From here, we'll import data into our Python environment in two different ways. First, let's try downloading the data from the website and then pulling it into this notebook. \n",
    "\n",
    "## Exercise 2\n",
    "Find the City Service Requests for 2016 in DC's Open Data Portal and download the spreadsheet (.csv file) to your computer.\n",
    "\n",
    "This is a data set containing requests for service through DC's 311 system. People can enter these requests by calling 311, texting DC-311 (32-311), using the 311 app, or visiting the city's online 311 portal. \n",
    "\n",
    "Now let's try pulling the data into our Python environment using Pandas. \n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Pull the .csv file into your Python environment using Pandas and assign it to an object called 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? You may have received an error like this: \"OSError: File b'City_Service_Requests_in_2016.csv' does not exist\"\n",
    "\n",
    "This means that the .csv file is not in the directory your notebook is calling from (which is, in fact, the directory the notebook is in). To find out which directory your notebook is calling from, import the os package and get the current working directory.\n",
    "\n",
    "## Exercise 4\n",
    "Import the os package and get the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what the current working directory is, we must either change the working directory or move the file into the current working directory. I'll let you decide which you want to do, but this is how you change the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\peter.casey\\\\Downloads') \n",
    "## There's this weird thing in Python where you have to use double slashes when specifying working directories.\n",
    "## If you have trouble getting a file from a directory you know it's in, this is a common error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to pull the data into your Jupyter Notebook. Try it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pull the .csv file directly from the Open Data Portal by using the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## I'm commenting this out because it takes a while to pull the data into the environment because it's a large file\n",
    "#df = pd.read_csv('https://opendata.arcgis.com/datasets/0e4b7d3a83b94a178b3d1f015db901ee_7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You have the data! \n",
    "\n",
    "One of the first things we'd like to know when we're dealing with a data set is its shape; that is the number of rows and columns it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has 302,925 rows and 30 columns. Generally speaking, rows are our \"observations\" or \"samples\", while columns are our \"variables\" or \"features\". \n",
    "\n",
    "Now try getting JUST the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have an item like this with multiple elements, you can often call the elements by their number. In Python, we always start counting elements with the number '0', so that the first element is always element '0'. \n",
    "\n",
    "## Exercise 5\n",
    "Now try getting the number of columns yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we may want to do is actually LOOK at the data. But you probably don't want to print out all 300,000 rows of data in your notebook! (In reality, Pandas won't do that. Instead, it will show you a subset of the rows.)\n",
    "\n",
    "But to have greater control we can use the head command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "The default head command shows us 5 rows. Try increasing the number of rows it shows us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is sort of any annoying way to look at the data, in my opinion. I'm usually interested in looking at the list of columns and the kinds of values they have, so I usually transpose the data when I print it as a head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can get a sense of what your data looks like. What do you think some of these columns mean?\n",
    "\n",
    "## Exercise 7\n",
    "Think about some of the column names and values you have. What do you think these columns are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want even more information about our variables or columns, so let's learn more about them.\n",
    "\n",
    "First, we can get a quick list of column names this way also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns command provides us with a list of column names.\n",
    "\n",
    "We can also use this command to find out how many columns we have by taking its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returns the same value as df.shape[1].\n",
    "\n",
    "We can also get column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "We see three data types here. What does each mean? Why aren't there any \"date\" types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to know even more about our data.\n",
    "\n",
    "## Exercise 9\n",
    "Use the describe function to learn more about your data. I like to transpose this, too. Try transposing it.\n",
    "\n",
    "Do you notice any columns missing from the output? Why do you think they're missing?\n",
    "\n",
    "You may notice some values are 'NaN'. What does that mean? How might we handle these values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe function provides us a ton of great information about numeric variables, like integers and floats. But categorical variables, called \"object\" variables in Python, do not have means, mins, maxes, or standard deviations. So how might we analyze these?\n",
    "\n",
    "One of the first steps is to take a look at the unique values of these columns. Let's start with the most interesting one: Service Code Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['SERVICECODEDESCRIPTION'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just did two new things right here: we called a column by using its column name, and we called its unique values.\n",
    "\n",
    "There's another way to call columns from a Pandas DataFrame if you're feeling lazy and don't want to mess around with brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.SERVICECODEDESCRIPTION.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "We can figure out how many unique values we have by getting the length of this object. Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get value counts for each unique value of a categorical variable using value_counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.SERVICECODEDESCRIPTION.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "What's the most common request? What does that request mean? Check out the District's online 311 portal at 311.dc.gov to learn more about the top service request. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
